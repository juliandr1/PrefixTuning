{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "3WMuta0IumOz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aOydYfgbjyoV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "58708b1e-0160-4c69-a5bd-be9d03cf781d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.40.1\n",
            "Uninstalling transformers-4.40.1:\n",
            "  Successfully uninstalled transformers-4.40.1\n",
            "Obtaining file:///content/drive/MyDrive/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from transformers==3.2.0) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from transformers==3.2.0) (24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==3.2.0) (3.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==3.2.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==3.2.0) (4.66.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==3.2.0) (2023.12.25)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.10/dist-packages (from transformers==3.2.0) (0.1.99)\n",
            "Collecting sacremoses (from transformers==3.2.0)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==3.2.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==3.2.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==3.2.0) (2024.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==3.2.0) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==3.2.0) (1.4.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building editable for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-3.2.0-0.editable-py3-none-any.whl size=15625 sha256=53f02dcfba9d0da7063b47d73e432cb28d538593772fb3d5616a0828473756d9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5baqic57/wheels/c4/5a/fe/c643984a08f166b105b12d0903475da42b7a23160c6d9ca879\n",
            "Successfully built transformers\n",
            "Installing collected packages: sacremoses, transformers\n",
            "Successfully installed sacremoses-0.1.1 transformers-3.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "092b5da1298e48efa53d30291be4dab8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip uninstall transformers -y\n",
        "!pip install -e /content/drive/MyDrive/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohx_OUv2hmno",
        "outputId": "2b7d59f6-3489-4afa-95d2-8998d8b342ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5Tokenizer\n",
        "\n",
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9Xca3blGhsFk"
      },
      "outputs": [],
      "source": [
        "import torch.nn.init as init\n",
        "class PrefixTuning(nn.Module):\n",
        "\n",
        "    def __init__(self, pretrained_config, prompt_len=48, hidden_dim = 800):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.match_n_layer = pretrained_config.num_layers\n",
        "        self.match_n_head = pretrained_config.num_heads\n",
        "        self.n_embd = pretrained_config.d_model\n",
        "        self.match_n_embd = self.n_embd // self.match_n_head\n",
        "\n",
        "        # Config of Pre-Trained LM\n",
        "        # torch.tensor([0, 1, 2, .. , prefix_len-1])\n",
        "        self.pretrained_config = pretrained_config\n",
        "        self.pre_prompt = torch.arange(prompt_len)\n",
        "\n",
        "        # Embedding\n",
        "        self.wte = nn.Embedding(num_embeddings=prompt_len, embedding_dim=self.n_embd)\n",
        "        # Reparameterization\n",
        "        self.control_trans = nn.Sequential(\n",
        "            nn.Linear(self.n_embd, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 2 * self.match_n_layer * self.n_embd)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.wte2 = nn.Embedding(num_embeddings=prompt_len, embedding_dim=self.n_embd)\n",
        "        # Reparameterization\n",
        "        self.control_trans2 = nn.Sequential(\n",
        "            nn.Linear(self.n_embd, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 2 * self.match_n_layer * self.n_embd)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.wte_enc = nn.Embedding(prompt_len, self.n_embd)\n",
        "        self.control_trans_enc = nn.Sequential(\n",
        "                        nn.Linear(self.n_embd, hidden_dim),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(hidden_dim, hidden_dim),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(hidden_dim, self.match_n_layer * 2 * self.n_embd))\n",
        "\n",
        "        self.prompt_len = prompt_len\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, batch_size, device, sample_size = 1):\n",
        "        # Shape: batch_size, prompt_len\n",
        "        input_tokens = self.pre_prompt.unsqueeze(0).expand(batch_size, -1).to(device)\n",
        "        # Shape: batch_size, prompt_len, d_model\n",
        "        temp_control = self.wte(input_tokens)\n",
        "        # Shape: batch_size, prompt_len, d_model\n",
        "        past_key_values = self.control_trans(temp_control)\n",
        "\n",
        "\n",
        "        temp_control2 = self.wte2(input_tokens)\n",
        "        past_key_values2 = self.control_trans2(temp_control2)  # bsz, seqlen, layer*emb\n",
        "\n",
        "        temp_control_enc = self.wte_enc(input_tokens)\n",
        "        past_key_values_enc = self.control_trans_enc(temp_control_enc)  # bsz, seqlen, layer*emb\n",
        "\n",
        "\n",
        "        if sample_size > 1:\n",
        "            past_key_values = torch.cat(sample_size * [past_key_values])\n",
        "\n",
        "        bsz, seqlen, _ = past_key_values.shape\n",
        "        past_key_values = past_key_values.view(bsz, seqlen, self.match_n_layer * 2, self.match_n_head,\n",
        "                                               self.match_n_embd)\n",
        "        past_key_values = self.dropout(past_key_values)\n",
        "        past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(2)\n",
        "\n",
        "        if sample_size > 1:\n",
        "            past_key_values2 = torch.cat(sample_size * [past_key_values2])\n",
        "\n",
        "        past_key_values2 = past_key_values2.view(bsz, seqlen, self.match_n_layer * 2, self.match_n_head,\n",
        "                                                   self.match_n_embd)\n",
        "        past_key_values2 = self.dropout(past_key_values2)\n",
        "        past_key_values2 = past_key_values2.permute([2, 0, 3, 1, 4]).split(2)\n",
        "\n",
        "\n",
        "        bsz_enc, seqlen, _ = past_key_values_enc.shape\n",
        "        past_key_values_enc = past_key_values_enc.view(bsz_enc, seqlen, self.match_n_layer * 2, self.match_n_head,\n",
        "                                                     self.match_n_embd)\n",
        "        past_key_values_enc = self.dropout(past_key_values_enc)\n",
        "        past_key_values_enc = past_key_values_enc.permute([2, 0, 3, 1, 4]).split(2)\n",
        "\n",
        "        result = []\n",
        "        for i, key_val in enumerate(past_key_values):\n",
        "            temp_dict = {'self': {\"prev_key\": key_val[0].contiguous(),\n",
        "                                  \"prev_value\": key_val[1].contiguous()\n",
        "                                 },\n",
        "                        }\n",
        "            key_val2 = past_key_values2[i]\n",
        "            temp_dict['encoder_decoder'] = {\"prev_key\": key_val2[0].contiguous(),\n",
        "                                                \"prev_value\": key_val2[1].contiguous()\n",
        "                                                }\n",
        "            key_val_enc = past_key_values_enc[i]\n",
        "            temp_dict['encoder'] = {\"prev_key\": key_val_enc[0].contiguous(),\n",
        "                                        \"prev_value\": key_val_enc[1].contiguous()\n",
        "                                        }\n",
        "            result.append(temp_dict)\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NlFPGf5vhtkI"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import unidecode\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "class LineByLineWebNLGTextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This will be superseded by a framework-agnostic approach\n",
        "    soon.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer: T5Tokenizer, file_path: str, eos_tok:str, is_eval = False):\n",
        "\n",
        "        with open(file_path) as f:\n",
        "            lines_dict = json.load(f)\n",
        "\n",
        "        full_src_lst = []\n",
        "        full_tgt_lst = []\n",
        "        full_rela_lst = []\n",
        "\n",
        "        for i, example in enumerate(lines_dict['entries']):\n",
        "            sents = example[str(i + 1)]['lexicalisations']\n",
        "            triples = example[str(i + 1)]['modifiedtripleset']\n",
        "            rela_lst = []\n",
        "            temp_triples = \"\"\n",
        "            for j, tripleset in enumerate(triples):\n",
        "                subj, rela, obj = tripleset['subject'], tripleset['property'], tripleset['object']\n",
        "                rela_lst.append(rela)\n",
        "                if i > 0:\n",
        "                  temp_triples += ' | '\n",
        "\n",
        "                temp_triples += '{} : {} : {}'.format(subj, rela, obj)\n",
        "\n",
        "            temp_triples = temp_triples.strip()\n",
        "\n",
        "            for sent in sents:\n",
        "                if sent[\"comment\"] == 'good':\n",
        "                    full_tgt_lst.append(sent[\"lex\"])\n",
        "                    full_src_lst.append(temp_triples)\n",
        "                    full_rela_lst.append(rela_lst)\n",
        "\n",
        "\n",
        "        assert len(full_rela_lst) == len(full_src_lst)\n",
        "        assert len(full_rela_lst) == len(full_tgt_lst)\n",
        "\n",
        "\n",
        "        full_src_list = [unidecode.unidecode(sent) for sent in full_src_lst]\n",
        "        full_tgt_list = [unidecode.unidecode(sent) for sent in full_tgt_lst]\n",
        "\n",
        "        srcs = []\n",
        "        tgts = []\n",
        "\n",
        "        for src, tgt in zip(full_src_lst, full_tgt_lst):\n",
        "            input = '{} {}'.format(src, eos_tok)\n",
        "            target = '{} {}'.format(tgt, eos_tok)\n",
        "            srcs.append(src)\n",
        "            tgts.append(tgt)\n",
        "\n",
        "        batch_encoding_src = tokenizer(srcs, add_special_tokens= True, is_split_into_words=False)\n",
        "        batch_encoding_tgt = tokenizer(tgts, add_special_tokens= True, is_split_into_words=False)\n",
        "\n",
        "        self.srcs = batch_encoding_src[\"input_ids\"]\n",
        "        self.labels = batch_encoding_tgt[\"input_ids\"]\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.srcs)\n",
        "\n",
        "    # def __getitem__(self, i) -> torch.Tensor:\n",
        "    def __getitem__(self, i):\n",
        "        return self.srcs[i], self.labels[i]\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "      max_len_data=0\n",
        "      max_len_label=0\n",
        "      for description, target in batch:\n",
        "          if len(description)>max_len_data: max_len_data=len(description)\n",
        "          if len(target)>max_len_label: max_len_label=len(target)\n",
        "\n",
        "      attn_masks=[]\n",
        "      targets=[]\n",
        "      descriptions=[]\n",
        "\n",
        "      for description, target in batch:\n",
        "          description.extend([self.tokenizer.pad_token_id]*(max_len_data-len(description)))\n",
        "          descriptions.append(description)\n",
        "\n",
        "          attn_mask=[int(e!=self.tokenizer.pad_token_id) for e in description]\n",
        "          attn_masks.append(attn_mask)\n",
        "\n",
        "          target.extend([-100]*(max_len_label-len(target)))\n",
        "          targets.append(target)\n",
        "\n",
        "      return torch.LongTensor(descriptions), torch.LongTensor(attn_masks), torch.LongTensor(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SBiheX9UrKdd"
      },
      "outputs": [],
      "source": [
        "def read_webnlg_files(path, tokenizer):\n",
        "    file_dict = {}\n",
        "\n",
        "    with open(path) as f:\n",
        "        lines_dict = json.load(f)\n",
        "\n",
        "    full_src_lst = []\n",
        "    total_count = 0\n",
        "\n",
        "    for i, example in enumerate(lines_dict['entries']):\n",
        "        sents = example[str(i + 1)]['lexicalisations']\n",
        "        triples = example[str(i + 1)]['modifiedtripleset']\n",
        "\n",
        "        temp_triples = \"\"\n",
        "        for j, tripleset in enumerate(triples):\n",
        "            subj, rela, obj = tripleset['subject'], tripleset['property'], tripleset['object']\n",
        "\n",
        "            if i > 0:\n",
        "              temp_triples += ' | '\n",
        "\n",
        "            temp_triples += '{} : {} : {}'.format(subj, rela, obj)\n",
        "\n",
        "        temp_triples = temp_triples.strip()\n",
        "        temp_triples = '{} {}'.format(temp_triples, tokenizer.eos_token)\n",
        "\n",
        "        for sent in sents:\n",
        "            if (temp_triples) not in file_dict:\n",
        "                file_dict[temp_triples] = []\n",
        "                full_src_lst.append(temp_triples)\n",
        "            file_dict[temp_triples].append(sent[\"lex\"])\n",
        "\n",
        "    return file_dict\n",
        "\n",
        "\n",
        "def write_e2e_corr(prompt_lst, file_dict, corr_path):\n",
        "    with open(corr_path, 'w') as f:\n",
        "        for x in prompt_lst:\n",
        "            for line in file_dict[x]:\n",
        "                if not line.strip():\n",
        "                    print('PROBLEM', line,'PROBLEM',file_dict[x] )\n",
        "                else:\n",
        "                    print(line, file=f)\n",
        "            print('', file=f)\n",
        "\n",
        "def write_e2e_src(prompt_lst, corr_path):\n",
        "    with open(corr_path, 'w') as f:\n",
        "        for x in prompt_lst:\n",
        "            print(x, file=f)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xsef5wOFD5fM"
      },
      "outputs": [],
      "source": [
        "!chmod +x /content/drive/MyDrive/web_nlg/evaluation/webnlg-automatic-evaluation/multi-bleu.perl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SsbNIh0yhqIu"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import unidecode\n",
        "class EvalTestDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This will be superseded by a framework-agnostic approach\n",
        "    soon.\n",
        "    \"\"\"\n",
        "    def __init__(self, tokenizer: T5Tokenizer, file_path: str):\n",
        "        self.prompt_text_dict = read_webnlg_files(file_path, tokenizer)\n",
        "        self.prompt_texts = list(self.prompt_text_dict.keys())\n",
        "        for i in range(len(self.prompt_texts)):\n",
        "            self.prompt_texts[i] = unidecode.unidecode(self.prompt_texts[i])\n",
        "\n",
        "        self.prompt_texts = tokenizer(self.prompt_texts, add_special_tokens=True, truncation=False,\n",
        "                                   is_split_into_words=False)['input_ids']\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.prompt_texts)\n",
        "\n",
        "    def __get_dict__(self):\n",
        "        return self.prompt_text_dict\n",
        "\n",
        "\n",
        "    # def __getitem__(self, i) -> torch.Tensor:\n",
        "    def __getitem__(self, i):\n",
        "        return self.prompt_texts[i]\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "      max_len_data=0\n",
        "      for description in batch:\n",
        "          if len(description)>max_len_data: max_len_data=len(description)\n",
        "\n",
        "      descriptions=[]\n",
        "      attn_masks=[]\n",
        "      for description in batch:\n",
        "          description.extend([self.tokenizer.pad_token_id]*(max_len_data-len(description)))\n",
        "          descriptions.append(description)\n",
        "\n",
        "          attn_mask=[int(e!=self.tokenizer.pad_token_id) for e in description]\n",
        "          attn_masks.append(attn_mask)\n",
        "\n",
        "      return torch.LongTensor(descriptions), torch.LongTensor(attn_masks)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_params(model: nn.Module):\n",
        "    \"\"\"Set requires_grad=False for each of model.parameters()\"\"\"\n",
        "    for par in model.parameters():\n",
        "        par.requires_grad = False"
      ],
      "metadata": {
        "id": "dQHbeiFeDbFX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import encode_long\n",
        "import json\n",
        "import yaml\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers.configuration_t5 import T5Config\n",
        "from transformers import T5Tokenizer, Adafactor, get_linear_schedule_with_warmup\n",
        "from transformers.modeling_t5 import T5ForConditionalGeneration\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "torch.manual_seed(101)\n",
        "\n",
        "prefix_size = 15\n",
        "batch_size = 5\n",
        "learning_rate = 5e-5\n",
        "epochs = 30\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "\n",
        "config = T5Config.from_pretrained('t5-base')\n",
        "config.use_prefix = True\n",
        "config.preseqlen = prefix_size\n",
        "\n",
        "    # Pre-Trained T5 Tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "    # Pre-Trained T5 Model\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-base', config=config).to(device)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "freeze_params(model.shared)\n",
        "for d in [model.encoder, model.decoder]:\n",
        "      freeze_params(d.embed_tokens)\n",
        "\n",
        "prefix_model = PrefixTuning(model.config, prefix_size).to(device)\n",
        "\n",
        "    # Initialize datasets and dataloaders\n",
        "dataset_train = LineByLineWebNLGTextDataset(\n",
        "        tokenizer,\n",
        "        \"/content/drive/MyDrive/web_nlg/train.json\",\n",
        "        tokenizer.eos_token)\n",
        "dataset_test = EvalTestDataset(tokenizer, \"/content/drive/MyDrive/web_nlg/test.json\")\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size= batch_size, shuffle=True, collate_fn=dataset_train.collate_fn)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size= batch_size, shuffle=False, collate_fn=dataset_test.collate_fn)\n",
        "\n",
        "total_training_steps = epochs * len(dataloader_train)\n",
        "optimizer = Adafactor(prefix_model.parameters(),\n",
        "                      lr=learning_rate,\n",
        "                      scale_parameter=False,\n",
        "                      relative_step=False)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=2000,\n",
        "    num_training_steps=total_training_steps,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3SaxSlXodVI",
        "outputId": "5da5f834-e1c9-4bcf-ff4d-a9bfd2cbb9b2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Init the T5ForConditionalGeneration Model with config.use_prefix=True, config.preseqlen=15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/transformers/src/transformers/tokenization_t5.py:184: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmYkpspohwLx",
        "outputId": "7ec7bac5-55a8-4292-af45-01fab7722e1c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 loss: 1.134943574799248\n",
            "Epoch 1 loss: 0.7463782113195292\n",
            "Epoch 2 loss: 0.6939649259994496\n",
            "Epoch 3 loss: 0.6631703717243985\n",
            "Files creating finished for:  model_base_lr5e-05_prefixlen100_epoch4\n",
            "ALL:\n",
            "SEEN:\n",
            "UNSEEN:\n",
            "Epoch 4 loss: 0.6402403124203795\n",
            "Epoch 5 loss: 0.6198029185259061\n",
            "Epoch 6 loss: 0.6031720535186392\n",
            "Epoch 7 loss: 0.5873453777433268\n",
            "Files creating finished for:  model_base_lr5e-05_prefixlen100_epoch8\n",
            "ALL:\n",
            "SEEN:\n",
            "UNSEEN:\n",
            "Epoch 8 loss: 0.5723025972403369\n",
            "Epoch 9 loss: 0.5605976353819261\n",
            "Epoch 10 loss: 0.5484320304182134\n"
          ]
        }
      ],
      "source": [
        "from pickle import encode_long\n",
        "import json\n",
        "import yaml\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers.configuration_t5 import T5Config\n",
        "from transformers import T5Tokenizer, Adafactor, get_linear_schedule_with_warmup\n",
        "from transformers.modeling_t5 import T5ForConditionalGeneration\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  prefix_model.train()\n",
        "  epoch_loss = 0\n",
        "\n",
        "  for step, (data, attention_mask, target) in enumerate(dataloader_train):\n",
        "      data = data.to(device)\n",
        "      attention_mask = attention_mask.to(device)\n",
        "      target = target.to(device)\n",
        "\n",
        "      prefix = prefix_model(batch_size = data.shape[0], device = device)\n",
        "\n",
        "      outputs = model(input_ids=data, attention_mask=attention_mask, labels=target, past_key_values=prefix, use_cache = False, use_prefix  = True)\n",
        "\n",
        "      loss = outputs[0]\n",
        "      loss.backward()\n",
        "\n",
        "      if (step + 1) % gradient_accumulation_steps == 0:\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        prefix_model.zero_grad()\n",
        "\n",
        "\n",
        "      epoch_loss += loss.item() * gradient_accumulation_steps\n",
        "\n",
        "  if (epoch % 4 == 0) and epoch != 0:\n",
        "    with torch.no_grad():\n",
        "      prefix_model.eval()\n",
        "\n",
        "      pred_file = f'/content/drive/MyDrive/web_nlg/preds/base/model_{\"base\"}_lr{learning_rate}_prefixlen{prefix_size}_epoch{epoch}.txt'\n",
        "      generated_seqs = []\n",
        "\n",
        "      for step, (data, attention_mask) in enumerate(dataloader_test):\n",
        "        data = data.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "\n",
        "        prefix = prefix_model(batch_size=data.shape[0], device=device, sample_size = 5)\n",
        "        outputs = model.generate(input_ids = data, attention_mask=attention_mask, early_stopping =  False, top_p = .9, num_beams=5, past_key_values = prefix, use_prefix = True, use_cache = True)\n",
        "        output_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        for i in range(len(output_texts)):\n",
        "          if (len(output_texts[i]) == 0):\n",
        "            generated_seqs.append(\"UNKNOWN TOKENS\")\n",
        "          else:\n",
        "            generated_seqs.append(output_texts[i])\n",
        "\n",
        "      write_e2e_src(generated_seqs, pred_file)\n",
        "      model_name = f'/content/drive/MyDrive/web_nlg/models/model_{\"base\"}_lr{learning_rate}_prefixlen{prefix_size}_epoch{epoch}.pt'\n",
        "      output_file = f'/content/drive/MyDrive/web_nlg/eval/base/model_{\"base\"}_lr{learning_rate}_prefixlen{prefix_size}_epoch{epoch}.txt'\n",
        "      name = f'model_{\"base\"}_lr{learning_rate}_prefixlen{prefix_size}_epoch{epoch}'\n",
        "\n",
        "      !bash /content/drive/MyDrive/web_nlg/evaluation/run_eval_on_webnlg.sh {pred_file} {output_file} {name} {\"test\"}\n",
        "      model_name = f'/content/drive/MyDrive/web_nlg/models/model_{\"base\"}_lr{learning_rate}_prefixlen{prefix_size}_epoch{epoch}.pt'\n",
        "      torch.save(prefix_model.state_dict(), model_name)\n",
        "  print(\"Epoch \" + str(epoch) + \" loss: \" + str(epoch_loss/len(dataloader_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "b7wH4AfGSwIx",
        "outputId": "23b58073-7418-4af1-8b4d-3375ad009a8c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'EvalTestDataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0f31bdbf42a1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvalTestDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/web_nlg/test.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataloader_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpred_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'/content/drive/MyDrive/web_nlg/preds/TEST1.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgenerated_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'EvalTestDataset' is not defined"
          ]
        }
      ],
      "source": [
        "dataset_test = EvalTestDataset(tokenizer, \"/content/drive/MyDrive/web_nlg/test.json\")\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, collate_fn=dataset_test.collate_fn)\n",
        "with torch.no_grad():\n",
        "    pred_file = f'/content/drive/MyDrive/web_nlg/preds/TEST1.txt'\n",
        "    generated_seqs = []\n",
        "    file_dict = dataset_test.__get_dict__()\n",
        "\n",
        "    for key in file_dict.keys():\n",
        "        generated_seqs.append(file_dict[key][0])\n",
        "\n",
        "    write_e2e_src(generated_seqs, pred_file)\n",
        "    output_file = f'/content/drive/MyDrive/web_nlg/eval/TEST_SCRIPT'\n",
        "    name = f'TEST'\n",
        "    !bash /content/drive/MyDrive/web_nlg/evaluation/run_eval_on_webnlg.sh {pred_file} {output_file} {name} {\"test\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd60F54DPK8B",
        "outputId": "71cb698a-1128-43ae-90d7-db9610ac0e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files creating finished for:  model_base_lr0.1_prefixlen50_epoch17\n",
            "ALL:\n",
            "SEEN:\n",
            "UNSEEN:\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(101)\n",
        "prefix_size = 50\n",
        "batch_size = 5\n",
        "\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-large').to(device)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "dataset_test = EvalTestDataset(tokenizer, \"/content/drive/MyDrive/web_nlg/test.json\")\n",
        "#dataset_test = LineByLineWebNLGTextDataset(\n",
        "#        tokenizer,\n",
        "#        \"/content/drive/MyDrive/web_nlg/test.json\",\n",
        "#        \"<start>\",\n",
        "#        tokenizer.eos_token)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, collate_fn=dataset_test.collate_fn)\n",
        "\n",
        "prefix_model = PrefixTuning(model.config, prefix_size).to(device)\n",
        "prefix_model.load_state_dict(torch.load(\"/content/drive/MyDrive/web_nlg/models/HELLOmodel_base_lr4e-5_prefixlen50_epoch3.pt\"))\n",
        "prefix_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred_file = f'/content/drive/MyDrive/web_nlg/preds/base/MORR1.txt'\n",
        "    generated_seqs = []\n",
        "    gold = []\n",
        "\n",
        "    for step, (data) in enumerate(dataloader_test):\n",
        "      data = data.to(device)\n",
        "      prefix = prefix_model(batch_size=data.shape[0], device=device)\n",
        "      outputs = model.generate(data, num_beams=5, prompt=prefix, length_penalty = 1.2)\n",
        "      output_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "      for i in range(len(output_texts)):\n",
        "        if (len(output_texts[i]) == 0):\n",
        "          generated_seqs.append(\"UNKNOWN TOKENS\")\n",
        "        else:\n",
        "          generated_seqs.append(output_texts[i])\n",
        "\n",
        "    write_e2e_src(generated_seqs, pred_file)\n",
        "    output_file = f'/content/drive/MyDrive/web_nlg/eval/MORR1.txt'\n",
        "    name = f'model_{\"base\"}_lr{.1}_prefixlen{prefix_size}_epoch{17}'\n",
        "    !bash /content/drive/MyDrive/web_nlg/evaluation/run_eval_on_webnlg.sh {pred_file} {output_file} {name} {\"test\"}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}